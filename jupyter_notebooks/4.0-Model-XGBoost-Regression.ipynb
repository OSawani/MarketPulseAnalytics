{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dash import Dash, html, dcc, Input, Output\n",
    "import plotly.io as pio\n",
    "import dash_bootstrap_components as dbc\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "# from sklearn.metrics import mean_squared_error# deprecated\n",
    "from sklearn.metrics import root_mean_squared_error,make_scorer# alternative\n",
    "# also import MAPE and relative_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from datetime import timedelta\n",
    "\n",
    "def extract_date_features(df):\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.sort_values(by='Date')\n",
    "    df['year'] = df['Date'].dt.year\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['day'] = df['Date'].dt.day\n",
    "    df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    # return df.drop(columns='Date')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder paths\n",
    "stock_prices_path = os.path.join('..', 'data', 'processed', 'stock_prices','processed_stock_prices.csv')\n",
    "df_stock_prices = pd.read_csv(stock_prices_path)\n",
    "df_stock_prices['Date'] = pd.to_datetime(df_stock_prices['Date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stocks Predictions using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_targets = {\n",
    "        'Volume': ['Open', 'High', 'Low', 'Close', 'year', 'month', 'day', 'day_of_week', 'is_weekend'#]\n",
    "                 ,'Open_Lag1','Open_Lag3','Open_Lag7','Close_Lag1','Close_Lag3','Close_Lag7','High_Lag1',\n",
    "                'High_Lag3','High_Lag7','Low_Lag1','Low_Lag3','Low_Lag7','Volume_Lag1','Volume_Lag3',\n",
    "                'Volume_Lag7','Open_MA3','Open_MA7','Close_MA3','Close_MA7','High_MA3','High_MA7',\n",
    "                'Low_MA3','Low_MA7','Volume_MA3','Volume_MA7'#]\n",
    "                ,'insider_TransactionValue_MA7','insider_TRANS_PRICEPERSHARE_Lag7','insider_TRANS_SHARES_Lag7',\n",
    "                'insider_TransactionValue_MA21','insider_TRANS_PRICEPERSHARE_Lag21','insider_TRANS_SHARES_Lag21',]\n",
    "    }\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [10,50,100,300],# This is the number of boosting rounds, or the number of trees in the model. risk overfitting if too high and will be computationally expensive.\n",
    "    'regressor__learning_rate': [0.01,0.05], # Also known as the “eta” parameter, the learning rate controls the impact of each tree on the final outcome.\n",
    "    # Lower values (e.g., 0.01 or 0.05) make the model train more conservatively, often requiring more n_estimators to reach an optimal fit, but they help prevent overfitting.\n",
    "    'regressor__max_depth': [2,4,8,16],# Limits the maximum depth of each decision tree.\n",
    "    'regressor__subsample': [0.5,0.7,0.9],# This parameter specifies the fraction of the training samples used to fit each individual tree\n",
    "    'regressor__colsample_bytree': [0.6, 0.8] # Defines the fraction of features (columns) to be randomly sampled for each tree.\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "symbol = 'AAPL'\n",
    "date_start = '2014-01-01'\n",
    "date_end = '2017-12-31'\n",
    "\n",
    "\n",
    "data = df_stock_prices[\n",
    "            (df_stock_prices['SYMBOL'] == symbol) &\n",
    "            (df_stock_prices['Date'] >= date_start) &\n",
    "            (df_stock_prices['Date'] <= date_end)\n",
    "        ].copy()\n",
    "data = extract_date_features(data)\n",
    "target = 'Volume'\n",
    "numerical_features = valid_targets[target]\n",
    "categorical_features = ['SYMBOL','Exists in Insiders','InsiderTransactionInLast7Days','InsiderTransactionInLast21Days']\n",
    "\n",
    "X = data.drop(target, axis=1)\n",
    "y = data[target]\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_features),\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the model with the XGBoost regressor\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=0)\n",
    "\n",
    "# Use a pipeline to handle preprocessing and modeling together\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', model)])\n",
    "# Set up TimeSeriesSplit for walk-forward validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)  # 5 splits in walk-forward fashion\n",
    "\n",
    "# # GridSearchCV with TimeSeriesSplit\n",
    "# grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=1, scoring='neg_mean_squared_error')\n",
    "rmse_scorer = make_scorer(lambda y_true, y_pred: root_mean_squared_error(y_true, y_pred), greater_is_better=False)\n",
    "mean_absolute_percentage_error_scorer = make_scorer(lambda y_true, y_pred: mean_absolute_percentage_error(y_true, y_pred), greater_is_better=False)\n",
    "\n",
    "# Update GridSearchCV to use RMSE as the scoring metric\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=1, scoring=mean_absolute_percentage_error_scorer)\n",
    "\n",
    "\n",
    "# Fit the grid search with walk-forward validation\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Display best hyperparameters and score\n",
    "print(\"Best hyperparameters found:\", grid_search.best_params_)\n",
    "print(\"Best CV score:\", -grid_search.best_score_)# CV stands for cross validation\n",
    "\n",
    "# Extract top 3 configurations\n",
    "top_3_configs = grid_search.cv_results_['params']\n",
    "top_3_scores = grid_search.cv_results_['mean_test_score']\n",
    "# calculate the top 3 R2 score \n",
    "\n",
    "\n",
    "# Combine scores and configs into a list of tuples and sort based on the score\n",
    "top_3_configs_sorted = sorted(zip(top_3_scores, top_3_configs), key=lambda x: x[0], reverse=True)[:3]\n",
    "\n",
    "# Display top 3 configurations\n",
    "for idx, (score, config) in enumerate(top_3_configs_sorted, 1):\n",
    "    print(f\"Rank {idx}: {config} with score: {-score:.2e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple stock analysis for the entire year of 2014\n",
    "    # Improved volume RMSE from 2.77e7 (linear regression) to 2.70e7 using XGBoost without lag features and moving averages\n",
    "    # Further improved volume RMSE from 2.70e7 to 2.35e7 using XGBoost with lag features and moving averages and insider\n",
    "\n",
    "# Apple stock analysis for the period from 2014 to 2017\n",
    "    # Improved volume RMSE from 2.14e7 (linear regression) to 2.13e7 using XGBoost without lag features and moving averages\n",
    "    # Further improved volume RMSE from 2.13e7 to 1.18e+07 using XGBoost with lag features and moving averages and insdier \n",
    "\n",
    "# Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
    "# Best hyperparameters found: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 4, 'regressor__n_estimators': 300, 'regressor__subsample': 0.9}\n",
    "# Best CV score: 11835991.344843375 (RMSE)\n",
    "# Rank 1: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 4, 'regressor__n_estimators': 300, 'regressor__subsample': 0.9} with score: 1.18e+07\n",
    "# Rank 2: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 4, 'regressor__n_estimators': 300, 'regressor__subsample': 0.7} with score: 1.21e+07\n",
    "# Rank 3: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 4, 'regressor__n_estimators': 100, 'regressor__subsample': 0.9} with score: 1.21e+07\n",
    "\n",
    "# Best hyperparameters found: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 4, 'regressor__n_estimators': 300, 'regressor__subsample': 0.9}\n",
    "# Best CV score: 0.2149187219244552 (MAPE)\n",
    "# Rank 1: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 4, 'regressor__n_estimators': 300, 'regressor__subsample': 0.9} with score: 2.15e-01\n",
    "# Rank 2: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 4, 'regressor__n_estimators': 300, 'regressor__subsample': 0.7} with score: 2.32e-01\n",
    "# Rank 3: {'regressor__colsample_bytree': 0.6, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 4, 'regressor__n_estimators': 300, 'regressor__subsample': 0.9} with score: 2.32e-01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "symbol = 'AAPL'\n",
    "date_start = '2014-01-01'\n",
    "date_end = '2017-12-31'\n",
    "\n",
    "\n",
    "data = df_stock_prices[\n",
    "            (df_stock_prices['SYMBOL'] == symbol) &\n",
    "            (df_stock_prices['Date'] >= date_start) &\n",
    "            (df_stock_prices['Date'] <= date_end)\n",
    "        ].copy()\n",
    "data = extract_date_features(data)\n",
    "target = 'Volume'\n",
    "numerical_features = valid_targets[target]\n",
    "categorical_features = ['SYMBOL','Exists in Insiders','InsiderTransactionInLast7Days','InsiderTransactionInLast21Days']\n",
    "\n",
    "X = data.drop(target, axis=1)\n",
    "y = data[target]\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_features),\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = {\n",
    "    'regressor__colsample_bytree': 0.8,\n",
    "    'regressor__learning_rate': 0.05,\n",
    "    'regressor__max_depth': 4,\n",
    "    'regressor__n_estimators': 300,\n",
    "    'regressor__subsample': 0.9\n",
    "}\n",
    "# Update model with best hyperparameters\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=0,\n",
    "    colsample_bytree=best_params['regressor__colsample_bytree'],\n",
    "    learning_rate=best_params['regressor__learning_rate'],\n",
    "    max_depth=best_params['regressor__max_depth'],\n",
    "    n_estimators=best_params['regressor__n_estimators'],\n",
    "    subsample=best_params['regressor__subsample']\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', model)])\n",
    "# Split data into training and testing (last 20%)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse = (root_mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE on the last 20% data: {rmse:.2e}\")\n",
    "# calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R2 score on the last 20% data: {r2:.2f}\")\n",
    "\n",
    "# Example prediction\n",
    "symbol = 'AAPL'\n",
    "date_start = '2017-01-01'\n",
    "date_end = '2017-12-31'\n",
    "\n",
    "new_data = df_stock_prices[\n",
    "    (df_stock_prices['SYMBOL'] == symbol) &\n",
    "    (df_stock_prices['Date'] >= date_start) &\n",
    "    (df_stock_prices['Date'] <= date_end)\n",
    "].copy()\n",
    "new_data = extract_date_features(new_data)\n",
    "X_new = new_data.drop('Volume', axis=1)\n",
    "predictions = pipeline.predict(X_new)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
