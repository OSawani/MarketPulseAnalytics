{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "Collect and preprocess raw data related to insider transactions and stock prices.\n",
    "Fetch data from Kaggle and save it as raw data, inspect and save it.\n",
    "\n",
    "**Inputs:**\n",
    "- Raw TSV files: `NONDERIV_TRANS.tsv`, `SUBMISSION.tsv`, `REPORTING_OWNER.tsv`\n",
    "- Stock price data files in `../data/raw/stock_prices/` directory\n",
    "- Kaggle JSON file - the authentication token.\n",
    "\n",
    "**Outputs:**\n",
    "- Interim CSV files:\n",
    "  - `interim_insider_transactions.csv`\n",
    "  - `interim_stock_prices.csv`\n",
    "  - `interim_merged_insider_transactions_stock_prices.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Imports & Kaggle Endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Access current directory and change to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir\n",
    "\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm Current Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Credintials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the .env file in the current directory (MarketPulseAnalytics)\n",
    "env_path = os.path.join(os.getcwd(), '.env')\n",
    "\n",
    "# Load environment variables from the .env file if it exists\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(env_path)\n",
    "    print(\".env file loaded from:\", env_path)\n",
    "else:\n",
    "    print(\"No .env file found in the current directory. Ensure environment variables are set in the hosting environment.\")\n",
    "\n",
    "# Access the environment variables\n",
    "kaggle_username = os.getenv('KAGGLE_USERNAME')\n",
    "kaggle_key = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "# Verify environment variables are set\n",
    "if not kaggle_username or not kaggle_key:\n",
    "    print(\"Warning: KAGGLE_USERNAME and/or KAGGLE_KEY environment variables are not set. Make sure they are configured in the production environment.\")\n",
    "else:\n",
    "    print(\"Environment variables loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Kaggle directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the download paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KaggleDatasetPath = \"borismarjanovic/price-volume-data-for-all-us-stocks-etfs\"\n",
    "DestinationFolder = \"data/raw/stock_prices/downloaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the downloaded file, and delete the zip file\n",
    "Define dataset, destination folder and download.\n",
    "Clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the downloaded ZIP file\n",
    "zip_file_path = os.path.join(DestinationFolder, \"price-volume-data-for-all-us-stocks-etfs.zip\")\n",
    "\n",
    "# Unzip the dataset\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(DestinationFolder)\n",
    "\n",
    "# Define paths for ETFs folder, Stocks folder, and target folder\n",
    "etfs_folder = os.path.join(DestinationFolder, \"ETFs\")\n",
    "stocks_folder = os.path.join(DestinationFolder, \"Stocks\")\n",
    "target_folder = os.path.join(\"data\", \"raw\", \"stock_prices\")\n",
    "\n",
    "# Delete the ETFs folder if it exists\n",
    "if os.path.exists(etfs_folder):\n",
    "    shutil.rmtree(etfs_folder)\n",
    "\n",
    "# Move the content of the \"Stocks\" folder to the target \"stock_prices\" folder\n",
    "if os.path.exists(stocks_folder):\n",
    "    for file_name in os.listdir(stocks_folder):\n",
    "        src_file = os.path.join(stocks_folder, file_name)\n",
    "        dest_file = os.path.join(target_folder, file_name)\n",
    "        \n",
    "        # Check if the destination file already exists\n",
    "        if os.path.exists(dest_file):\n",
    "            print(f\"File {file_name} already exists in the target directory. Skipping.\")\n",
    "        else:\n",
    "            shutil.move(src_file, dest_file)\n",
    "\n",
    "    # Remove the now-empty \"Stocks\" folder\n",
    "    shutil.rmtree(stocks_folder)\n",
    "\n",
    "# Delete the original ZIP file if it exists\n",
    "if os.path.exists(zip_file_path):\n",
    "    os.remove(zip_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Files in stock_prices:\", os.listdir(target_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the project root explicitly as \"MarketPulseAnalytics\" within \"Repo\"\n",
    "base_dir = os.path.dirname(os.getcwd())  # Go up one level to \"Repo\"\n",
    "project_root = os.path.join(base_dir, \"Repo\", \"MarketPulseAnalytics\")  # Set \"MarketPulseAnalytics\" as project root\n",
    "\n",
    "# Change to the project root directory\n",
    "os.chdir(project_root)\n",
    "print(\"New current directory:\", os.getcwd())\n",
    "\n",
    "# Define paths relative to the project root\n",
    "downloaded_folder = os.path.join(\"data\", \"raw\", \"insider_transactions\", \"downloaded\")\n",
    "extracted_folder = os.path.join(\"data\", \"raw\", \"insider_transactions\")\n",
    "\n",
    "# Ensure both the extracted folders exist\n",
    "os.makedirs(extracted_folder, exist_ok=True)\n",
    "\n",
    "# List all ZIP files in the downloaded folder\n",
    "zip_files = [f for f in os.listdir(downloaded_folder) if f.endswith('.zip')]\n",
    "\n",
    "if not zip_files:\n",
    "    print(\"No ZIP files found in the downloaded folder.\")\n",
    "else:\n",
    "    # Iterate over each ZIP file\n",
    "    for zip_file in zip_files:\n",
    "        zip_file_path = os.path.join(downloaded_folder, zip_file)\n",
    "        \n",
    "        # Create a folder named after the ZIP file (without the .zip extension) in the extracted folder\n",
    "        zip_folder_name = os.path.splitext(zip_file)[0]\n",
    "        destination_folder = os.path.join(extracted_folder, zip_folder_name)\n",
    "        os.makedirs(destination_folder, exist_ok=True)\n",
    "        print(f\"Extracting {zip_file} into {destination_folder}...\")\n",
    "\n",
    "        # Extract ZIP file into the specific folder\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(destination_folder)\n",
    "\n",
    "    print(\"All ZIP files have been extracted into respective folders and deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Insider Trading ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a: NONDERIV_TRANS.TSV (from different quearters and years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all files and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the files\n",
    "files = [os.path.join('..', 'data', 'raw', 'insider_transactions', f'{year}q{quarter}_form345', 'NONDERIV_TRANS.tsv')\n",
    "         for year in range(2014, 2018) for quarter in range(1, 5)]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df = pd.concat(dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed (ensure columns exist)(either columns have so many missing values or they are not needed)\n",
    "columns_to_drop = ['DIRECT_INDIRECT_OWNERSHIP_FN',\n",
    "                   'NATURE_OF_OWNERSHIP',\n",
    "                   'NATURE_OF_OWNERSHIP_FN',\n",
    "                   'VALU_OWND_FOLWNG_TRANS',\n",
    "                   'VALU_OWND_FOLWNG_TRANS_FN',                   \n",
    "                   'SHRS_OWND_FOLWNG_TRANS_FN',\n",
    "                   'TRANS_ACQUIRED_DISP_CD_FN',\n",
    "                   'TRANS_PRICEPERSHARE_FN',\n",
    "                   'TRANS_SHARES_FN',\n",
    "                   'TRANS_TIMELINESS_FN',\n",
    "                   'EQUITY_SWAP_TRANS_CD_FN',\n",
    "                   'TRANS_CODE',\n",
    "                   'TRANS_FORM_TYPE',\n",
    "                   'DEEMED_EXECUTION_DATE_FN',\n",
    "                   'DEEMED_EXECUTION_DATE',\n",
    "                   'TRANS_DATE_FN',\n",
    "                   'SECURITY_TITLE_FN',\n",
    "                   'SECURITY_TITLE']\n",
    "# Drop columns if they exist in the DataFrame\n",
    "df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "# Function to correct the year format\n",
    "def correct_year_format(date_str):\n",
    "    match = re.match(r'(\\d{2}-\\w{3}-00(\\d{2}))', date_str)\n",
    "    if match:\n",
    "        corrected_year = date_str.replace('00', '20', 1)  # Replace the leading '00' with '20'\n",
    "        return corrected_year\n",
    "    return date_str\n",
    "\n",
    "# Apply the function to the TRANS_DATE column\n",
    "df['TRANS_DATE'] = df['TRANS_DATE'].apply(correct_year_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjust column values mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for column EQUITY_SWAP_INVOLVED, 0 = false, 1 = true\n",
    "df['EQUITY_SWAP_INVOLVED'] = df['EQUITY_SWAP_INVOLVED'].astype(str)\n",
    "print(df['EQUITY_SWAP_INVOLVED'].unique())\n",
    "# Map the column values to ensure consistent True/False representation\n",
    "df['EQUITY_SWAP_INVOLVED'] = df['EQUITY_SWAP_INVOLVED'].replace({\n",
    "    'false': 'False',\n",
    "    '0': 'False',\n",
    "    '1': 'True',\n",
    "    'true': 'True',\n",
    "    'False': 'False',\n",
    "    'True': 'True'\n",
    "})\n",
    "# Convert the column to boolean type\n",
    "df['EQUITY_SWAP_INVOLVED'] = df['EQUITY_SWAP_INVOLVED'].map({'True': True, 'False': False})\n",
    "# Print unique values to confirm conversion\n",
    "print(df['EQUITY_SWAP_INVOLVED'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for column TRANS_TIMELINESS, E = early, L = late, O = on time\n",
    "print(df['TRANS_TIMELINESS'].unique())\n",
    "df['TRANS_TIMELINESS'] = df['TRANS_TIMELINESS'].replace(np.nan, 'O')\n",
    "print(df['TRANS_TIMELINESS'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove rows where SHRS_OWND_FOLWING_TRANS is nan or TRANS_PRICEPERSHR is nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the column SHRS_OWND_FOLWNG_TRANS and TRANS_PRICEPERSHARE we remove any rows where the value is NaN for either column\n",
    "df = df.dropna(subset=['SHRS_OWND_FOLWNG_TRANS', 'TRANS_PRICEPERSHARE'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print the dataframe summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANS_ACQUIRED_DISP_CD: A = acquired, D = disposed\n",
    "# DIRECT_INDIRECT_OWNERSHIP: D = direct, I = indirect\n",
    "# EQUITY_SWAP_INVOLVED: 0 = false, 1 = true\n",
    "\n",
    "# Print DataFrame information\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b: SUBMISSION.TSV (from different quearters and years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all files and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the files\n",
    "files = [os.path.join('..', 'data', 'raw', 'insider_transactions', f'{year}q{quarter}_form345', 'SUBMISSION.tsv')\n",
    "         for year in range(2014, 2018) for quarter in range(1, 5)]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df2 = pd.concat(dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coulmns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep columns: ACCESSION_NUMBER, FILING_DATE, PERIOD_OF_REPORT, ISSUERNAME, ISSUERTRADINGSYMBOL\n",
    "columns_to_keep = ['ACCESSION_NUMBER', 'FILING_DATE', 'PERIOD_OF_REPORT', 'ISSUERNAME', 'ISSUERTRADINGSYMBOL']\n",
    "# Drop columns that are not needed\n",
    "df2.drop(columns=[col for col in df2.columns if col not in columns_to_keep], inplace=True)\n",
    "# Print DataFrame information\n",
    "print(df2.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same company name should have the same trading symbol\n",
    "# if 'ISSUERTRADINGSYMBOL' is nan, we look at its corresponding ISSUERNAME value. \n",
    "# if the corresponding ISSUERNAME is not nan, we can use it to find other rows of the same ISSUERNAME where ISSUERTRADINGSYMBOL is not nan and fill the nan value with the non-nan value.\n",
    "\n",
    "#  if ISSUERNAME is nan, we can't do anything about it. we will just leave it as nan and drop rows where ISSUERTRADINGSYMBOL is nan \n",
    "\n",
    "# Create a mapping of ISSUERNAME to ISSUERTRADINGSYMBOL for non-NaN trading symbols\n",
    "issuer_symbol_map = df2.dropna(subset=['ISSUERTRADINGSYMBOL']).set_index('ISSUERNAME')['ISSUERTRADINGSYMBOL'].to_dict()\n",
    "\n",
    "# Apply the mapping to fill NaN values in ISSUERTRADINGSYMBOL based on ISSUERNAME\n",
    "df2['ISSUERTRADINGSYMBOL'] = df2.apply(\n",
    "    lambda row: issuer_symbol_map.get(row['ISSUERNAME'], row['ISSUERTRADINGSYMBOL']) \n",
    "    if pd.isna(row['ISSUERTRADINGSYMBOL']) and pd.notna(row['ISSUERNAME']) else row['ISSUERTRADINGSYMBOL'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop rows where ISSUERTRADINGSYMBOL is still NaN\n",
    "df2.dropna(subset=['ISSUERTRADINGSYMBOL'], inplace=True)\n",
    "\n",
    "# Print DataFrame info to verify changes\n",
    "# FILING_DATE is when the form was filed to the commission\n",
    "# TRANS_DATE is when the transaction was executed\n",
    "# declaration of intent to trade or smth like that means that PERIOD_OF_REPORT can be before or same data as TRANS_DATE\n",
    "# while filing date is maybe not needed for predictions, the report period can be useful.\n",
    "# we can check if the report period is done before transaction date, indicating clear intent to trade . (maybe we can use delta between the two dates as a feature)\n",
    "\n",
    "print(df2.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c: REPORTING_OWNER.tsv (from different quearters and years)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all files and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the files\n",
    "files = [os.path.join('..', 'data', 'raw', 'insider_transactions', f'{year}q{quarter}_form345', 'REPORTINGOWNER.tsv')\n",
    "         for year in range(2014, 2018) for quarter in range(1, 5)]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df3 = pd.concat(dataframes, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep RPTOWNER_RELATIONSHIP and ACCESSION_NUMBER\n",
    "columns_to_keep = ['RPTOWNER_RELATIONSHIP', 'ACCESSION_NUMBER']\n",
    "# Drop columns that are not needed\n",
    "df3.drop(columns=[col for col in df3.columns if col not in columns_to_keep], inplace=True)\n",
    "#drop nan RPTOWNER_RELATIONSHIP\n",
    "df3.dropna(subset=['RPTOWNER_RELATIONSHIP'], inplace=True)\n",
    "# Print DataFrame information\n",
    "print(df3.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d: Joined NONDERIV_TRANS.tsv, SUBMISSION.tsv, REPORTING_OWNER.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4= join df, df2, df3 on ACCESSION_NUMBER\n",
    "df4 = df.merge(df2, on='ACCESSION_NUMBER').merge(df3, on='ACCESSION_NUMBER')\n",
    "# Print DataFrame information\n",
    "print(df4.info())\n",
    "# TRANS_ACQUIRED_DISP_CD: A = acquired, D = disposed\n",
    "# DIRECT_INDIRECT_OWNERSHIP: D = direct, I = indirect\n",
    "# EQUITY_SWAP_INVOLVED: 0 = false, 1 = true\n",
    "# for column TRANS_TIMELINESS, E = early, L = late, O = on time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Stock Prices ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date,Open,High,Low,Close,Volume,OpenInt\n",
    "#  above are the columns in the stock data. We can ignore the OpenInt column as it is not needed.\n",
    "# the folder structure is ../data/raw/stock_data/xxx.us.txt where xxx is the stock symbol.\n",
    "# before the first '.' delimiter, we have the symbol name.\n",
    "# After the second '.' delimiter, we have the country name (us in this case). \n",
    "#  Therefore, the Insider Trading data's ISSUERTRADINGSYMBOL column should match the stock symbol name. The country name here seems irrelevant since in the REPORTINGOWNER.tsv file, we have the country name but that country is like the address of reporting owner (person) and not the stock.\n",
    "#  Therefore, we can ignore the country name in the stock data file name and just match the symbol name.\n",
    "\n",
    "#  let's read all files and extract the symbol name from the file name and store it in a new column called 'SYMBOL'. We will remove OpenInt column as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a list of all the files\n",
    "files = [os.path.join('..', 'data', 'raw', 'stock_prices', filename) \n",
    "         for filename in os.listdir(os.path.join('..', 'data', 'raw', 'stock_prices')) \n",
    "         if filename.endswith('.txt')]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    #  extract symbol name from file name which is the string before the first '.' delimiter in the file name\n",
    "    symbol = os.path.basename(file).split('.')[0]\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep=',', low_memory=False)\n",
    "            # Add a new column 'SYMBOL' with the symbol name\n",
    "            temp['SYMBOL'] = symbol\n",
    "            # remove OpenInt column\n",
    "            temp.drop(columns=['OpenInt'], inplace=True)\n",
    "            # filter dates to be from 2014 till 2017 (inclusive and all months)\n",
    "            temp = temp[temp['Date'].str.startswith('2014') | temp['Date'].str.startswith('2015') | temp['Date'].str.startswith('2016') | temp['Date'].str.startswith('2017')]\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')# as an example: Error reading ..\\data\\raw\\stock_prices\\accp.us.txt: No columns to parse from file (empty data file)\n",
    "\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df5 = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Merging Insider Trading and Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the stocks prices dataset has  Symbol and Date columns (Date and SYMBOL)\n",
    "#  the insider trading data has the transaction date and the stock symbol name (TRANS_DATE and ISSUERTRADINGSYMBOL)TRANS_DATE HAS THE FORM 13-MAR-2014\n",
    "\n",
    "# therefore, we can join the insider trading data with the stock prices data on the stock symbol name and the transaction date. dATE HAS THE FORM 2014-01-23    \n",
    "# df4 is the insider trading data and df5 is the stock prices data\n",
    "\n",
    "# Ensure both dataframes have symbol columns in the same case (e.g., uppercase)\n",
    "df4['ISSUERTRADINGSYMBOL'] = df4['ISSUERTRADINGSYMBOL'].str.upper()\n",
    "df5['SYMBOL'] = df5['SYMBOL'].str.upper()\n",
    "\n",
    "# Convert TRANS_DATE to the same format as Date in df5\n",
    "df4['TRANS_DATE'] = pd.to_datetime(df4['TRANS_DATE'], format='%d-%b-%Y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge the insider trading data with the stock prices data on the stock symbol name and the transaction date\n",
    "merged_df = pd.merge(df4, df5, left_on=['ISSUERTRADINGSYMBOL', 'TRANS_DATE'], right_on=['SYMBOL', 'Date'], how='inner')\n",
    "\n",
    "# Print the merged DataFrame information\n",
    "print(merged_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Saving Interim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 1,322,820 million rows for all the insider trading data files.(7,877 unique symbols)\n",
    "# We have 5,442,556 rows for the stocks price data files. (7,163 unique symbols)\n",
    "# merging both based on the stock symbol name and the transaction date, we have 978,647 rows. (4,450 unique symbols)\n",
    "# from 2014 to 2017, we have 1,043 working business days.\n",
    "\n",
    "# naturally, the insider trading data is less than the stock prices data as not all companies have insider trading data.\n",
    "# the merged data could be useful for predicting stock prices based on insider trading data.(direct daily relationship between insider trading data and stock prices)\n",
    "# but there will be many more data points in the stock prices that have no corresponding insider trading data. (indirect relationship between insider trading data and stock prices).\n",
    "# in our plot, we can first plot all stocks prices and then color-code the points that have insider trading data vs those that don't have insider trading data.\n",
    "\n",
    "\n",
    "#  for now, let's save the df4,  to the folder path ../data/interim/insider_transactions\n",
    "#  let's save the df5 to the folder path ../data/interim/stock_prices\n",
    "# let's save the merged_df to the folder path ../data/interim/merged_insider_transactions_stock_prices\n",
    "# we save using paths and os packages that work on all operating systems.\n",
    "#  if the folders do not exist, we create them.\n",
    "\n",
    "# Define the folder paths\n",
    "insider_transactions_path = os.path.join('..', 'data', 'interim', 'insider_transactions')\n",
    "stock_prices_path = os.path.join('..', 'data', 'interim', 'stock_prices')\n",
    "merged_path = os.path.join('..', 'data', 'interim', 'merged_insider_transactions_stock_prices')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(insider_transactions_path, exist_ok=True)\n",
    "os.makedirs(stock_prices_path, exist_ok=True)\n",
    "os.makedirs(merged_path, exist_ok=True)\n",
    "\n",
    "# Save the DataFrames to the respective paths\n",
    "df4.to_csv(os.path.join(insider_transactions_path, 'interim_insider_transactions.csv'), index=False)\n",
    "df5.to_csv(os.path.join(stock_prices_path, 'interim_stock_prices.csv'), index=False)\n",
    "merged_df.to_csv(os.path.join(merged_path, 'interim_merged_insider_transactions_stock_prices.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
