{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "Collect and preprocess raw data related to insider transactions and stock prices.\n",
    "Fetch data from Kaggle and save it as raw data, inspect and save it.\n",
    "\n",
    "**Inputs:**\n",
    "- Raw TSV files: `NONDERIV_TRANS.tsv`, `SUBMISSION.tsv`, `REPORTING_OWNER.tsv`\n",
    "- Stock price data files in `../data/raw/stock_prices/` directory\n",
    "- Kaggle JSON file - the authentication token.\n",
    "\n",
    "**Outputs:**\n",
    "- Interim CSV files:\n",
    "  - `interim_insider_transactions.csv`\n",
    "  - `interim_stock_prices.csv`\n",
    "  - `interim_merged_insider_transactions_stock_prices.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Imports & Kaggle Endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import zipfile\n",
    "import shutil\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Access current directory and change to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir\n",
    "\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm Current Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\sawaomar\\\\Documents\\\\Project 5\\\\Repo 2\\\\MarketPulseAnalytics'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Credintials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env file loaded from: c:\\Users\\sawaomar\\Documents\\Project 5\\Repo 2\\MarketPulseAnalytics\\.env\n",
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the .env file in the current directory (MarketPulseAnalytics)\n",
    "env_path = os.path.join(os.getcwd(), '.env')\n",
    "\n",
    "# Load environment variables from the .env file if it exists\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(env_path)\n",
    "    print(\".env file loaded from:\", env_path)\n",
    "else:\n",
    "    print(\"No .env file found in the current directory. Ensure environment variables are set in the hosting environment.\")\n",
    "\n",
    "# Access the environment variables\n",
    "kaggle_username = os.getenv('KAGGLE_USERNAME')\n",
    "kaggle_key = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "# Verify environment variables are set\n",
    "if not kaggle_username or not kaggle_key:\n",
    "    print(\"Warning: KAGGLE_USERNAME and/or KAGGLE_KEY environment variables are not set. Make sure they are configured in the production environment.\")\n",
    "else:\n",
    "    print(\"Environment variables loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the download paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "stock_prices_download_path = 'data/downloaded/zip_stock_prices/'\n",
    "stock_prices_filename = \"price-volume-data-for-all-us-stocks-etfs.zip\"\n",
    "stock_prices_unzip_path = 'data/raw/stock_prices/'\n",
    "\n",
    "insider_transactions_download_path = 'data/downloaded/zip_insider_transactions/'\n",
    "insider_transactions_filename = 'sec-insider-transactions.zip'\n",
    "insider_transactions_unzip_path = 'data/raw/insider_transactions/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the dataset name\n",
    "stock_prices_dataset = \"borismarjanovic/price-volume-data-for-all-us-stocks-etfs\"\n",
    "insider_transactions_dataset = \"osawani/sec-insider-transactions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the dataset using Kaggle CLI\n",
    "def download_dataset(dataset_name, download_path):\n",
    "    # Create the Kaggle CLI command as a string\n",
    "    command = f\"kaggle datasets download -d {dataset_name} -p {download_path}\"\n",
    "    \n",
    "    # Print the command for debugging purposes\n",
    "    print(f\"Running command: {command}\")\n",
    "    \n",
    "    # Use os.system to run the command (works in a regular Python script)\n",
    "    os.system(command)\n",
    "    \n",
    "    # Notify the user the download is complete\n",
    "    print(f\"Dataset {dataset_name} downloaded successfully to {download_path}\")\n",
    "\n",
    "# Function to check if the file exists in the folder and download it if it doesn't\n",
    "def check_and_download_file(folder_path, filename, dataset_name):\n",
    "    # Construct the full path to the file\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File {filename} already exists in {folder_path}\")\n",
    "        return True  # File exists\n",
    "    else:\n",
    "        print(f\"File {filename} does NOT exist in {folder_path}. Downloading now...\")\n",
    "        download_dataset(dataset_name, folder_path)  # Download the dataset\n",
    "        return False  # File does not exist, and download started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to unzip the Stock Prices dataset (only the 'Stocks' folder)\n",
    "\n",
    "\n",
    "def unzip_stock_prices(zip_path, unzip_path, specific_folder='Stocks'):\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Error: {zip_path} does not exist. Please download the zip file first.\")\n",
    "        return\n",
    "\n",
    "    # Get the directory of the zip file to extract the Stocks folder in the same location\n",
    "    zip_dir = os.path.dirname(zip_path)\n",
    "    \n",
    "    print(f\"Unzipping {zip_path} to {zip_dir}...\")\n",
    "\n",
    "    # Extract the Stocks folder to the same location as the zip file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # List all files in the zip\n",
    "        all_files = zip_ref.namelist()\n",
    "\n",
    "        # Filter for files that start with the specific folder name (e.g., 'Stocks/')\n",
    "        files_to_extract = [file for file in all_files if file.startswith(specific_folder)]\n",
    "        \n",
    "        # Extract the files\n",
    "        for file in files_to_extract:\n",
    "            zip_ref.extract(file, zip_dir)\n",
    "    \n",
    "    # Path to the extracted Stocks folder\n",
    "    stocks_folder_path = os.path.join(zip_dir, specific_folder)\n",
    "\n",
    "    # Ensure the Stocks folder exists before moving its contents\n",
    "    if os.path.exists(stocks_folder_path):\n",
    "        # Move all contents of the Stocks folder to the target directory\n",
    "        for item in os.listdir(stocks_folder_path):\n",
    "            source = os.path.join(stocks_folder_path, item)\n",
    "            destination = os.path.join(unzip_path, item)\n",
    "            \n",
    "            # Move files or directories\n",
    "            if os.path.isdir(source):\n",
    "                shutil.move(source, destination)\n",
    "            else:\n",
    "                shutil.move(source, destination)\n",
    "        \n",
    "        # Delete the Stocks folder after moving its contents\n",
    "        shutil.rmtree(stocks_folder_path)\n",
    "        \n",
    "        print(f\"Moved contents from '{stocks_folder_path}' to '{unzip_path}' and deleted the folder.\")\n",
    "    else:\n",
    "        print(f\"Error: The folder '{specific_folder}' was not found in the zip file.\")\n",
    "\n",
    "    print(f\"Unzipping complete. Files extracted and moved to {unzip_path}.\")\n",
    "\n",
    "# Function to unzip the Insider Transactions dataset (extract everything)\n",
    "def unzip_insider_transactions(zip_path, unzip_path):\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Error: {zip_path} does not exist. Please download the zip file first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Unzipping {zip_path} to {unzip_path}...\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Extract all files\n",
    "        zip_ref.extractall(unzip_path)\n",
    "\n",
    "    print(f\"Unzipping complete. Files extracted to {unzip_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File price-volume-data-for-all-us-stocks-etfs.zip already exists in data/downloaded/zip_stock_prices/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Check and download the Stock Prices dataset if the file doesn't exist\n",
    "check_and_download_file(stock_prices_download_path, stock_prices_filename, stock_prices_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sec-insider-transactions.zip already exists in data/downloaded/zip_insider_transactions/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 2. Check and download the Insider Transactions dataset if the file doesn't exist\n",
    "check_and_download_file(insider_transactions_download_path, insider_transactions_filename, insider_transactions_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping data/downloaded/zip_stock_prices/price-volume-data-for-all-us-stocks-etfs.zip to data/downloaded/zip_stock_prices...\n",
      "Moved contents from 'data/downloaded/zip_stock_prices\\Stocks' to 'data/raw/stock_prices/' and deleted the folder.\n",
      "Unzipping complete. Files extracted and moved to data/raw/stock_prices/.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Unzip the Stock Prices dataset (only the 'Stocks' folder)\n",
    "zip_stock_prices_path = os.path.join(stock_prices_download_path, stock_prices_filename)\n",
    "unzip_stock_prices(zip_stock_prices_path, stock_prices_unzip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping data/downloaded/zip_insider_transactions/sec-insider-transactions.zip to data/raw/insider_transactions/...\n",
      "Unzipping complete. Files extracted to data/raw/insider_transactions/.\n",
      "Download and extraction complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Unzip the Insider Transactions dataset (all contents)\n",
    "zip_insider_transactions_path = os.path.join(insider_transactions_download_path, insider_transactions_filename)\n",
    "unzip_insider_transactions(zip_insider_transactions_path, insider_transactions_unzip_path)\n",
    "\n",
    "print(\"Download and extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Insider Trading ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a: NONDERIV_TRANS.TSV (from different quearters and years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all files and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the files\n",
    "files = [os.path.join('data', 'raw', 'insider_transactions', f'{year}q{quarter}_form345', 'NONDERIV_TRANS.tsv')\n",
    "         for year in range(2014, 2018) for quarter in range(1, 5)]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed (ensure columns exist)(either columns have so many missing values or they are not needed)\n",
    "columns_to_drop = ['DIRECT_INDIRECT_OWNERSHIP_FN',\n",
    "                   'NATURE_OF_OWNERSHIP',\n",
    "                   'NATURE_OF_OWNERSHIP_FN',\n",
    "                   'VALU_OWND_FOLWNG_TRANS',\n",
    "                   'VALU_OWND_FOLWNG_TRANS_FN',                   \n",
    "                   'SHRS_OWND_FOLWNG_TRANS_FN',\n",
    "                   'TRANS_ACQUIRED_DISP_CD_FN',\n",
    "                   'TRANS_PRICEPERSHARE_FN',\n",
    "                   'TRANS_SHARES_FN',\n",
    "                   'TRANS_TIMELINESS_FN',\n",
    "                   'EQUITY_SWAP_TRANS_CD_FN',\n",
    "                   'TRANS_CODE',\n",
    "                   'TRANS_FORM_TYPE',\n",
    "                   'DEEMED_EXECUTION_DATE_FN',\n",
    "                   'DEEMED_EXECUTION_DATE',\n",
    "                   'TRANS_DATE_FN',\n",
    "                   'SECURITY_TITLE_FN',\n",
    "                   'SECURITY_TITLE']\n",
    "# Drop columns if they exist in the DataFrame\n",
    "df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "# Function to correct the year format\n",
    "def correct_year_format(date_str):\n",
    "    match = re.match(r'(\\d{2}-\\w{3}-00(\\d{2}))', date_str)\n",
    "    if match:\n",
    "        corrected_year = date_str.replace('00', '20', 1)  # Replace the leading '00' with '20'\n",
    "        return corrected_year\n",
    "    return date_str\n",
    "\n",
    "# Apply the function to the TRANS_DATE column\n",
    "df['TRANS_DATE'] = df['TRANS_DATE'].apply(correct_year_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjust column values mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' 'false' 'true' '1']\n",
      "[False  True]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for column EQUITY_SWAP_INVOLVED, 0 = false, 1 = true\n",
    "df['EQUITY_SWAP_INVOLVED'] = df['EQUITY_SWAP_INVOLVED'].astype(str)\n",
    "print(df['EQUITY_SWAP_INVOLVED'].unique())\n",
    "# Map the column values to ensure consistent True/False representation\n",
    "df['EQUITY_SWAP_INVOLVED'] = df['EQUITY_SWAP_INVOLVED'].replace({\n",
    "    'false': 'False',\n",
    "    '0': 'False',\n",
    "    '1': 'True',\n",
    "    'true': 'True',\n",
    "    'False': 'False',\n",
    "    'True': 'True'\n",
    "})\n",
    "# Convert the column to boolean type\n",
    "df['EQUITY_SWAP_INVOLVED'] = df['EQUITY_SWAP_INVOLVED'].map({'True': True, 'False': False})\n",
    "# Print unique values to confirm conversion\n",
    "print(df['EQUITY_SWAP_INVOLVED'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'L' 'E']\n",
      "['O' 'L' 'E']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for column TRANS_TIMELINESS, E = early, L = late, O = on time\n",
    "print(df['TRANS_TIMELINESS'].unique())\n",
    "df['TRANS_TIMELINESS'] = df['TRANS_TIMELINESS'].replace(np.nan, 'O')\n",
    "print(df['TRANS_TIMELINESS'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove rows where SHRS_OWND_FOLWING_TRANS is nan or TRANS_PRICEPERSHR is nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ACCESSION_NUMBER  NONDERIV_TRANS_SK   TRANS_DATE  \\\n",
      "0        0001209191-14-023991            2222416  13-MAR-2014   \n",
      "1        0001209191-14-023991            2222417  27-MAR-2014   \n",
      "2        0000950142-14-000771            2246926  31-MAR-2014   \n",
      "3        0000950142-14-000771            2246925  31-MAR-2014   \n",
      "4        0000950142-14-000771            2246924  31-MAR-2014   \n",
      "...                       ...                ...          ...   \n",
      "1211550  0000899243-17-023050            1183621  30-SEP-2017   \n",
      "1211551  0001437749-17-016566            1345652  28-SEP-2017   \n",
      "1211552  0001437749-17-016566            1345651  28-SEP-2017   \n",
      "1211553  0001437749-17-016565            1287918  08-SEP-2017   \n",
      "1211554  0001437749-17-016565            1287919  28-SEP-2017   \n",
      "\n",
      "         EQUITY_SWAP_INVOLVED TRANS_TIMELINESS  TRANS_SHARES  \\\n",
      "0                       False                O      31279.00   \n",
      "1                       False                O       6264.00   \n",
      "2                       False                O     991679.00   \n",
      "3                       False                O    1547085.00   \n",
      "4                       False                O    4461236.00   \n",
      "...                       ...              ...           ...   \n",
      "1211550                 False                O      77271.13   \n",
      "1211551                 False                O       7563.00   \n",
      "1211552                 False                O       7563.00   \n",
      "1211553                 False                O      22444.00   \n",
      "1211554                 False                O      22444.00   \n",
      "\n",
      "         TRANS_PRICEPERSHARE TRANS_ACQUIRED_DISP_CD  SHRS_OWND_FOLWNG_TRANS  \\\n",
      "0                       4.27                      D               255816.00   \n",
      "1                       4.30                      D               249552.00   \n",
      "2                      11.00                      D              3363796.00   \n",
      "3                      11.00                      D              5247739.00   \n",
      "4                      11.00                      D             35280202.00   \n",
      "...                      ...                    ...                     ...   \n",
      "1211550                 9.71                      D               106262.21   \n",
      "1211551                61.76                      D                    0.00   \n",
      "1211552                23.34                      A                 7563.00   \n",
      "1211553                23.34                      A                22444.00   \n",
      "1211554                61.13                      D                    0.00   \n",
      "\n",
      "        DIRECT_INDIRECT_OWNERSHIP  \n",
      "0                               D  \n",
      "1                               D  \n",
      "2                               D  \n",
      "3                               D  \n",
      "4                               D  \n",
      "...                           ...  \n",
      "1211550                         D  \n",
      "1211551                         D  \n",
      "1211552                         D  \n",
      "1211553                         D  \n",
      "1211554                         D  \n",
      "\n",
      "[1145840 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# using the column SHRS_OWND_FOLWNG_TRANS and TRANS_PRICEPERSHARE we remove any rows where the value is NaN for either column\n",
    "df = df.dropna(subset=['SHRS_OWND_FOLWNG_TRANS', 'TRANS_PRICEPERSHARE'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print the dataframe summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1145840 entries, 0 to 1211554\n",
      "Data columns (total 10 columns):\n",
      " #   Column                     Non-Null Count    Dtype  \n",
      "---  ------                     --------------    -----  \n",
      " 0   ACCESSION_NUMBER           1145840 non-null  object \n",
      " 1   NONDERIV_TRANS_SK          1145840 non-null  int64  \n",
      " 2   TRANS_DATE                 1145840 non-null  object \n",
      " 3   EQUITY_SWAP_INVOLVED       1145840 non-null  bool   \n",
      " 4   TRANS_TIMELINESS           1145840 non-null  object \n",
      " 5   TRANS_SHARES               1145840 non-null  float64\n",
      " 6   TRANS_PRICEPERSHARE        1145840 non-null  float64\n",
      " 7   TRANS_ACQUIRED_DISP_CD     1145840 non-null  object \n",
      " 8   SHRS_OWND_FOLWNG_TRANS     1145840 non-null  float64\n",
      " 9   DIRECT_INDIRECT_OWNERSHIP  1145840 non-null  object \n",
      "dtypes: bool(1), float64(3), int64(1), object(5)\n",
      "memory usage: 88.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TRANS_ACQUIRED_DISP_CD: A = acquired, D = disposed\n",
    "# DIRECT_INDIRECT_OWNERSHIP: D = direct, I = indirect\n",
    "# EQUITY_SWAP_INVOLVED: 0 = false, 1 = true\n",
    "\n",
    "# Print DataFrame information\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b: SUBMISSION.TSV (from different quearters and years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all files and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the files\n",
    "files = [os.path.join('data', 'raw', 'insider_transactions', f'{year}q{quarter}_form345', 'SUBMISSION.tsv')\n",
    "         for year in range(2014, 2018) for quarter in range(1, 5)]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df2 = pd.concat(dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coulmns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 843327 entries, 0 to 843326\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   ACCESSION_NUMBER     843327 non-null  object\n",
      " 1   FILING_DATE          843327 non-null  object\n",
      " 2   PERIOD_OF_REPORT     843327 non-null  object\n",
      " 3   ISSUERNAME           843327 non-null  object\n",
      " 4   ISSUERTRADINGSYMBOL  840899 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 32.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# We only keep columns: ACCESSION_NUMBER, FILING_DATE, PERIOD_OF_REPORT, ISSUERNAME, ISSUERTRADINGSYMBOL\n",
    "columns_to_keep = ['ACCESSION_NUMBER', 'FILING_DATE', 'PERIOD_OF_REPORT', 'ISSUERNAME', 'ISSUERTRADINGSYMBOL']\n",
    "# Drop columns that are not needed\n",
    "df2.drop(columns=[col for col in df2.columns if col not in columns_to_keep], inplace=True)\n",
    "# Print DataFrame information\n",
    "print(df2.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 841817 entries, 0 to 843326\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   ACCESSION_NUMBER     841817 non-null  object\n",
      " 1   FILING_DATE          841817 non-null  object\n",
      " 2   PERIOD_OF_REPORT     841817 non-null  object\n",
      " 3   ISSUERNAME           841817 non-null  object\n",
      " 4   ISSUERTRADINGSYMBOL  841817 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 38.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the same company name should have the same trading symbol\n",
    "# if 'ISSUERTRADINGSYMBOL' is nan, we look at its corresponding ISSUERNAME value. \n",
    "# if the corresponding ISSUERNAME is not nan, we can use it to find other rows of the same ISSUERNAME where ISSUERTRADINGSYMBOL is not nan and fill the nan value with the non-nan value.\n",
    "\n",
    "#  if ISSUERNAME is nan, we can't do anything about it. we will just leave it as nan and drop rows where ISSUERTRADINGSYMBOL is nan \n",
    "\n",
    "# Create a mapping of ISSUERNAME to ISSUERTRADINGSYMBOL for non-NaN trading symbols\n",
    "issuer_symbol_map = df2.dropna(subset=['ISSUERTRADINGSYMBOL']).set_index('ISSUERNAME')['ISSUERTRADINGSYMBOL'].to_dict()\n",
    "\n",
    "# Apply the mapping to fill NaN values in ISSUERTRADINGSYMBOL based on ISSUERNAME\n",
    "df2['ISSUERTRADINGSYMBOL'] = df2.apply(\n",
    "    lambda row: issuer_symbol_map.get(row['ISSUERNAME'], row['ISSUERTRADINGSYMBOL']) \n",
    "    if pd.isna(row['ISSUERTRADINGSYMBOL']) and pd.notna(row['ISSUERNAME']) else row['ISSUERTRADINGSYMBOL'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop rows where ISSUERTRADINGSYMBOL is still NaN\n",
    "df2.dropna(subset=['ISSUERTRADINGSYMBOL'], inplace=True)\n",
    "\n",
    "# Print DataFrame info to verify changes\n",
    "# FILING_DATE is when the form was filed to the commission\n",
    "# TRANS_DATE is when the transaction was executed\n",
    "# declaration of intent to trade or smth like that means that PERIOD_OF_REPORT can be before or same data as TRANS_DATE\n",
    "# while filing date is maybe not needed for predictions, the report period can be useful.\n",
    "# we can check if the report period is done before transaction date, indicating clear intent to trade . (maybe we can use delta between the two dates as a feature)\n",
    "\n",
    "print(df2.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c: REPORTING_OWNER.tsv (from different quearters and years)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all files and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the files\n",
    "files = [os.path.join('data', 'raw', 'insider_transactions', f'{year}q{quarter}_form345', 'REPORTINGOWNER.tsv')\n",
    "         for year in range(2014, 2018) for quarter in range(1, 5)]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df3 = pd.concat(dataframes, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 914301 entries, 0 to 914387\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count   Dtype \n",
      "---  ------                 --------------   ----- \n",
      " 0   ACCESSION_NUMBER       914301 non-null  object\n",
      " 1   RPTOWNER_RELATIONSHIP  914301 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 20.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# only keep RPTOWNER_RELATIONSHIP and ACCESSION_NUMBER\n",
    "columns_to_keep = ['RPTOWNER_RELATIONSHIP', 'ACCESSION_NUMBER']\n",
    "# Drop columns that are not needed\n",
    "df3.drop(columns=[col for col in df3.columns if col not in columns_to_keep], inplace=True)\n",
    "#drop nan RPTOWNER_RELATIONSHIP\n",
    "df3.dropna(subset=['RPTOWNER_RELATIONSHIP'], inplace=True)\n",
    "# Print DataFrame information\n",
    "print(df3.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d: Joined NONDERIV_TRANS.tsv, SUBMISSION.tsv, REPORTING_OWNER.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1322820 entries, 0 to 1322819\n",
      "Data columns (total 15 columns):\n",
      " #   Column                     Non-Null Count    Dtype  \n",
      "---  ------                     --------------    -----  \n",
      " 0   ACCESSION_NUMBER           1322820 non-null  object \n",
      " 1   NONDERIV_TRANS_SK          1322820 non-null  int64  \n",
      " 2   TRANS_DATE                 1322820 non-null  object \n",
      " 3   EQUITY_SWAP_INVOLVED       1322820 non-null  bool   \n",
      " 4   TRANS_TIMELINESS           1322820 non-null  object \n",
      " 5   TRANS_SHARES               1322820 non-null  float64\n",
      " 6   TRANS_PRICEPERSHARE        1322820 non-null  float64\n",
      " 7   TRANS_ACQUIRED_DISP_CD     1322820 non-null  object \n",
      " 8   SHRS_OWND_FOLWNG_TRANS     1322820 non-null  float64\n",
      " 9   DIRECT_INDIRECT_OWNERSHIP  1322820 non-null  object \n",
      " 10  FILING_DATE                1322820 non-null  object \n",
      " 11  PERIOD_OF_REPORT           1322820 non-null  object \n",
      " 12  ISSUERNAME                 1322820 non-null  object \n",
      " 13  ISSUERTRADINGSYMBOL        1322820 non-null  object \n",
      " 14  RPTOWNER_RELATIONSHIP      1322820 non-null  object \n",
      "dtypes: bool(1), float64(3), int64(1), object(10)\n",
      "memory usage: 142.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# df4= join df, df2, df3 on ACCESSION_NUMBER\n",
    "df4 = df.merge(df2, on='ACCESSION_NUMBER').merge(df3, on='ACCESSION_NUMBER')\n",
    "# Print DataFrame information\n",
    "print(df4.info())\n",
    "# TRANS_ACQUIRED_DISP_CD: A = acquired, D = disposed\n",
    "# DIRECT_INDIRECT_OWNERSHIP: D = direct, I = indirect\n",
    "# EQUITY_SWAP_INVOLVED: 0 = false, 1 = true\n",
    "# for column TRANS_TIMELINESS, E = early, L = late, O = on time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Stock Prices ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading data\\raw\\stock_prices\\accp.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\amrh.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\amrhw.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\asns.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\bbrx.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\bolt.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\boxl.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\bxg.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\ehr.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\fmax.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\gnst.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\hayu.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\jt.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\mapi.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\molc.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\otg.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\pbio.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\pxus.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\rbio.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\sail.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\sbt.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\scci.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\scph.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\send.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\sfix.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\srva.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\stnl.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\vist.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\vmet.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\wnfm.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\wspt.us.txt: No columns to parse from file\n",
      "Error reading data\\raw\\stock_prices\\znwaa.us.txt: No columns to parse from file\n"
     ]
    }
   ],
   "source": [
    "# Date,Open,High,Low,Close,Volume,OpenInt\n",
    "#  above are the columns in the stock data. We can ignore the OpenInt column as it is not needed.\n",
    "# the folder structure is ../data/raw/stock_data/xxx.us.txt where xxx is the stock symbol.\n",
    "# before the first '.' delimiter, we have the symbol name.\n",
    "# After the second '.' delimiter, we have the country name (us in this case). \n",
    "#  Therefore, the Insider Trading data's ISSUERTRADINGSYMBOL column should match the stock symbol name. The country name here seems irrelevant since in the REPORTINGOWNER.tsv file, we have the country name but that country is like the address of reporting owner (person) and not the stock.\n",
    "#  Therefore, we can ignore the country name in the stock data file name and just match the symbol name.\n",
    "\n",
    "#  let's read all files and extract the symbol name from the file name and store it in a new column called 'SYMBOL'. We will remove OpenInt column as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a list of all the files\n",
    "files = [os.path.join('data', 'raw', 'stock_prices', filename) \n",
    "         for filename in os.listdir(os.path.join('data', 'raw', 'stock_prices')) \n",
    "         if filename.endswith('.txt')]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    #  extract symbol name from file name which is the string before the first '.' delimiter in the file name\n",
    "    symbol = os.path.basename(file).split('.')[0]\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep=',', low_memory=False)\n",
    "            # Add a new column 'SYMBOL' with the symbol name\n",
    "            temp['SYMBOL'] = symbol\n",
    "            # remove OpenInt column\n",
    "            temp.drop(columns=['OpenInt'], inplace=True)\n",
    "            # filter dates to be from 2014 till 2017 (inclusive and all months)\n",
    "            temp = temp[temp['Date'].str.startswith('2014') | temp['Date'].str.startswith('2015') | temp['Date'].str.startswith('2016') | temp['Date'].str.startswith('2017')]\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')# as an example: Error reading ..\\data\\raw\\stock_prices\\accp.us.txt: No columns to parse from file (empty data file)\n",
    "\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df5 = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Merging Insider Trading and Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 978647 entries, 0 to 978646\n",
      "Data columns (total 22 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   ACCESSION_NUMBER           978647 non-null  object \n",
      " 1   NONDERIV_TRANS_SK          978647 non-null  int64  \n",
      " 2   TRANS_DATE                 978647 non-null  object \n",
      " 3   EQUITY_SWAP_INVOLVED       978647 non-null  bool   \n",
      " 4   TRANS_TIMELINESS           978647 non-null  object \n",
      " 5   TRANS_SHARES               978647 non-null  float64\n",
      " 6   TRANS_PRICEPERSHARE        978647 non-null  float64\n",
      " 7   TRANS_ACQUIRED_DISP_CD     978647 non-null  object \n",
      " 8   SHRS_OWND_FOLWNG_TRANS     978647 non-null  float64\n",
      " 9   DIRECT_INDIRECT_OWNERSHIP  978647 non-null  object \n",
      " 10  FILING_DATE                978647 non-null  object \n",
      " 11  PERIOD_OF_REPORT           978647 non-null  object \n",
      " 12  ISSUERNAME                 978647 non-null  object \n",
      " 13  ISSUERTRADINGSYMBOL        978647 non-null  object \n",
      " 14  RPTOWNER_RELATIONSHIP      978647 non-null  object \n",
      " 15  Date                       978647 non-null  object \n",
      " 16  Open                       978647 non-null  float64\n",
      " 17  High                       978647 non-null  float64\n",
      " 18  Low                        978647 non-null  float64\n",
      " 19  Close                      978647 non-null  float64\n",
      " 20  Volume                     978647 non-null  int64  \n",
      " 21  SYMBOL                     978647 non-null  object \n",
      "dtypes: bool(1), float64(7), int64(2), object(12)\n",
      "memory usage: 157.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#  the stocks prices dataset has  Symbol and Date columns (Date and SYMBOL)\n",
    "#  the insider trading data has the transaction date and the stock symbol name (TRANS_DATE and ISSUERTRADINGSYMBOL)TRANS_DATE HAS THE FORM 13-MAR-2014\n",
    "\n",
    "# therefore, we can join the insider trading data with the stock prices data on the stock symbol name and the transaction date. dATE HAS THE FORM 2014-01-23    \n",
    "# df4 is the insider trading data and df5 is the stock prices data\n",
    "\n",
    "# Ensure both dataframes have symbol columns in the same case (e.g., uppercase)\n",
    "df4['ISSUERTRADINGSYMBOL'] = df4['ISSUERTRADINGSYMBOL'].str.upper()\n",
    "df5['SYMBOL'] = df5['SYMBOL'].str.upper()\n",
    "\n",
    "# Convert TRANS_DATE to the same format as Date in df5\n",
    "df4['TRANS_DATE'] = pd.to_datetime(df4['TRANS_DATE'], format='%d-%b-%Y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge the insider trading data with the stock prices data on the stock symbol name and the transaction date\n",
    "merged_df = pd.merge(df4, df5, left_on=['ISSUERTRADINGSYMBOL', 'TRANS_DATE'], right_on=['SYMBOL', 'Date'], how='inner')\n",
    "\n",
    "# Print the merged DataFrame information\n",
    "print(merged_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Saving Interim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 1,322,820 million rows for all the insider trading data files.(7,877 unique symbols)\n",
    "# We have 5,442,556 rows for the stocks price data files. (7,163 unique symbols)\n",
    "# merging both based on the stock symbol name and the transaction date, we have 978,647 rows. (4,450 unique symbols)\n",
    "# from 2014 to 2017, we have 1,043 working business days.\n",
    "\n",
    "# naturally, the insider trading data is less than the stock prices data as not all companies have insider trading data.\n",
    "# the merged data could be useful for predicting stock prices based on insider trading data.(direct daily relationship between insider trading data and stock prices)\n",
    "# but there will be many more data points in the stock prices that have no corresponding insider trading data. (indirect relationship between insider trading data and stock prices).\n",
    "# in our plot, we can first plot all stocks prices and then color-code the points that have insider trading data vs those that don't have insider trading data.\n",
    "\n",
    "\n",
    "#  for now, let's save the df4,  to the folder path ../data/interim/insider_transactions\n",
    "#  let's save the df5 to the folder path ../data/interim/stock_prices\n",
    "# let's save the merged_df to the folder path ../data/interim/merged_insider_transactions_stock_prices\n",
    "# we save using paths and os packages that work on all operating systems.\n",
    "#  if the folders do not exist, we create them.\n",
    "\n",
    "# Define the folder paths\n",
    "insider_transactions_path = os.path.join('data', 'interim', 'insider_transactions')\n",
    "stock_prices_path = os.path.join('data', 'interim', 'stock_prices')\n",
    "merged_path = os.path.join('data', 'interim', 'merged_insider_transactions_stock_prices')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(insider_transactions_path, exist_ok=True)\n",
    "os.makedirs(stock_prices_path, exist_ok=True)\n",
    "os.makedirs(merged_path, exist_ok=True)\n",
    "\n",
    "# Save the DataFrames to the respective paths\n",
    "df4.to_csv(os.path.join(insider_transactions_path, 'interim_insider_transactions.csv'), index=False)\n",
    "df5.to_csv(os.path.join(stock_prices_path, 'interim_stock_prices.csv'), index=False)\n",
    "merged_df.to_csv(os.path.join(merged_path, 'interim_merged_insider_transactions_stock_prices.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
