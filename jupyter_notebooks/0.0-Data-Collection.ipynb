{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "Collect and preprocess raw data related to insider transactions and stock prices.\n",
    "Fetch data from Kaggle and save it as raw data, inspect and save it.\n",
    "\n",
    "**Inputs:**\n",
    "- Raw TSV files: `NONDERIV_TRANS.tsv`, `SUBMISSION.tsv`, `REPORTING_OWNER.tsv`\n",
    "- Stock price data files in `../data/raw/stock_prices/` directory\n",
    "- Kaggle JSON file - the authentication token.\n",
    "\n",
    "**Outputs:**\n",
    "- Interim CSV files:\n",
    "  - `interim_insider_transactions.csv`\n",
    "  - `interim_stock_prices.csv`\n",
    "  - `interim_merged_insider_transactions_stock_prices.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Imports & Kaggle Endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import zipfile\n",
    "import shutil\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Access current directory and change to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir\n",
    "\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm Current Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Credintials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the .env file in the current directory (MarketPulseAnalytics)\n",
    "env_path = os.path.join(os.getcwd(), '.env')\n",
    "\n",
    "# Load environment variables from the .env file if it exists\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(env_path)\n",
    "    print(\".env file loaded from:\", env_path)\n",
    "else:\n",
    "    print(\"No .env file found in the current directory. Ensure environment variables are set in the hosting environment.\")\n",
    "\n",
    "# Access the environment variables\n",
    "kaggle_username = os.getenv('KAGGLE_USERNAME')\n",
    "kaggle_key = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "# Verify environment variables are set\n",
    "if not kaggle_username or not kaggle_key:\n",
    "    print(\"Warning: KAGGLE_USERNAME and/or KAGGLE_KEY environment variables are not set. Make sure they are configured in the production environment.\")\n",
    "else:\n",
    "    print(\"Environment variables loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the download paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "stock_prices_download_path = 'data/downloaded/zip_stock_prices/'\n",
    "stock_prices_filename = \"price-volume-data-for-all-us-stocks-etfs.zip\"\n",
    "stock_prices_unzip_path = 'data/raw/stock_prices/'\n",
    "\n",
    "insider_transactions_download_path = 'data/downloaded/zip_insider_transactions/'\n",
    "insider_transactions_filename = 'sec-insider-transactions.zip'\n",
    "insider_transactions_unzip_path = 'data/raw/insider_transactions/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the dataset name\n",
    "stock_prices_dataset = \"borismarjanovic/price-volume-data-for-all-us-stocks-etfs\"\n",
    "insider_transactions_dataset = \"osawani/sec-insider-transactions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the dataset using Kaggle CLI\n",
    "def download_dataset(dataset_name, download_path):\n",
    "    # Create the Kaggle CLI command as a string\n",
    "    command = f\"kaggle datasets download -d {dataset_name} -p {download_path}\"\n",
    "    \n",
    "    # Print the command for debugging purposes\n",
    "    print(f\"Running command: {command}\")\n",
    "    \n",
    "    # Use os.system to run the command (works in a regular Python script)\n",
    "    os.system(command)\n",
    "    \n",
    "    # Notify the user the download is complete\n",
    "    print(f\"Dataset {dataset_name} downloaded successfully to {download_path}\")\n",
    "\n",
    "# Function to check if the file exists in the folder and download it if it doesn't\n",
    "def check_and_download_file(folder_path, filename, dataset_name):\n",
    "    # Construct the full path to the file\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File {filename} already exists in {folder_path}\")\n",
    "        return True  # File exists\n",
    "    else:\n",
    "        print(f\"File {filename} does NOT exist in {folder_path}. Downloading now...\")\n",
    "        download_dataset(dataset_name, folder_path)  # Download the dataset\n",
    "        return False  # File does not exist, and download started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to unzip the Stock Prices dataset (only the 'Stocks' folder)\n",
    "\n",
    "\n",
    "def unzip_stock_prices(zip_path, unzip_path, specific_folder='Stocks'):\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Error: {zip_path} does not exist. Please download the zip file first.\")\n",
    "        return\n",
    "\n",
    "    # Get the directory of the zip file to extract the Stocks folder in the same location\n",
    "    zip_dir = os.path.dirname(zip_path)\n",
    "    \n",
    "    print(f\"Unzipping {zip_path} to {zip_dir}...\")\n",
    "\n",
    "    # Extract the Stocks folder to the same location as the zip file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # List all files in the zip\n",
    "        all_files = zip_ref.namelist()\n",
    "\n",
    "        # Filter for files that start with the specific folder name (e.g., 'Stocks/')\n",
    "        files_to_extract = [file for file in all_files if file.startswith(specific_folder)]\n",
    "        \n",
    "        # Extract the files\n",
    "        for file in files_to_extract:\n",
    "            zip_ref.extract(file, zip_dir)\n",
    "    \n",
    "    # Path to the extracted Stocks folder\n",
    "    stocks_folder_path = os.path.join(zip_dir, specific_folder)\n",
    "\n",
    "    # Ensure the Stocks folder exists before moving its contents\n",
    "    if os.path.exists(stocks_folder_path):\n",
    "        # Move all contents of the Stocks folder to the target directory\n",
    "        for item in os.listdir(stocks_folder_path):\n",
    "            source = os.path.join(stocks_folder_path, item)\n",
    "            destination = os.path.join(unzip_path, item)\n",
    "            \n",
    "            # Move files or directories\n",
    "            if os.path.isdir(source):\n",
    "                shutil.move(source, destination)\n",
    "            else:\n",
    "                shutil.move(source, destination)\n",
    "        \n",
    "        # Delete the Stocks folder after moving its contents\n",
    "        shutil.rmtree(stocks_folder_path)\n",
    "        \n",
    "        print(f\"Moved contents from '{stocks_folder_path}' to '{unzip_path}' and deleted the folder.\")\n",
    "    else:\n",
    "        print(f\"Error: The folder '{specific_folder}' was not found in the zip file.\")\n",
    "\n",
    "    print(f\"Unzipping complete. Files extracted and moved to {unzip_path}.\")\n",
    "\n",
    "# Function to unzip the Insider Transactions dataset (extract everything)\n",
    "def unzip_insider_transactions(zip_path, unzip_path):\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Error: {zip_path} does not exist. Please download the zip file first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Unzipping {zip_path} to {unzip_path}...\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Extract all files\n",
    "        zip_ref.extractall(unzip_path)\n",
    "\n",
    "    print(f\"Unzipping complete. Files extracted to {unzip_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check and download the Stock Prices dataset if the file doesn't exist\n",
    "check_and_download_file(stock_prices_download_path, stock_prices_filename, stock_prices_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Check and download the Insider Transactions dataset if the file doesn't exist\n",
    "check_and_download_file(insider_transactions_download_path, insider_transactions_filename, insider_transactions_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Unzip the Stock Prices dataset (only the 'Stocks' folder)\n",
    "zip_stock_prices_path = os.path.join(stock_prices_download_path, stock_prices_filename)\n",
    "unzip_stock_prices(zip_stock_prices_path, stock_prices_unzip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Unzip the Insider Transactions dataset (all contents)\n",
    "zip_insider_transactions_path = os.path.join(insider_transactions_download_path, insider_transactions_filename)\n",
    "unzip_insider_transactions(zip_insider_transactions_path, insider_transactions_unzip_path)\n",
    "\n",
    "print(\"Download and extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Insider Trading ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a: NONDERIV_TRANS.TSV (from different quearters and years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all files and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the files\n",
    "files = [os.path.join('data', 'raw', 'insider_transactions', f'{year}q{quarter}_form345', 'NONDERIV_TRANS.tsv')\n",
    "         for year in range(2014, 2018) for quarter in range(1, 5)]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed (ensure columns exist)(either columns have so many missing values or they are not needed)\n",
    "columns_to_drop = ['DIRECT_INDIRECT_OWNERSHIP_FN',\n",
    "                   'NATURE_OF_OWNERSHIP',\n",
    "                   'NATURE_OF_OWNERSHIP_FN',\n",
    "                   'VALU_OWND_FOLWNG_TRANS',\n",
    "                   'VALU_OWND_FOLWNG_TRANS_FN',                   \n",
    "                   'SHRS_OWND_FOLWNG_TRANS_FN',\n",
    "                   'TRANS_ACQUIRED_DISP_CD_FN',\n",
    "                   'TRANS_PRICEPERSHARE_FN',\n",
    "                   'TRANS_SHARES_FN',\n",
    "                   'TRANS_TIMELINESS_FN',\n",
    "                   'EQUITY_SWAP_TRANS_CD_FN',\n",
    "                   'TRANS_CODE',\n",
    "                   'TRANS_FORM_TYPE',\n",
    "                   'DEEMED_EXECUTION_DATE_FN',\n",
    "                   'DEEMED_EXECUTION_DATE',\n",
    "                   'TRANS_DATE_FN',\n",
    "                   'SECURITY_TITLE_FN',\n",
    "                   'SECURITY_TITLE']\n",
    "# Drop columns if they exist in the DataFrame\n",
    "df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "# Function to correct the year format\n",
    "def correct_year_format(date_str):\n",
    "    match = re.match(r'(\\d{2}-\\w{3}-00(\\d{2}))', date_str)\n",
    "    if match:\n",
    "        corrected_year = date_str.replace('00', '20', 1)  # Replace the leading '00' with '20'\n",
    "        return corrected_year\n",
    "    return date_str\n",
    "\n",
    "# Apply the function to the TRANS_DATE column\n",
    "df['TRANS_DATE'] = df['TRANS_DATE'].apply(correct_year_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjust column values mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for column EQUITY_SWAP_INVOLVED, 0 = false, 1 = true\n",
    "df['EQUITY_SWAP_INVOLVED'] = df['EQUITY_SWAP_INVOLVED'].astype(str)\n",
    "print(df['EQUITY_SWAP_INVOLVED'].unique())\n",
    "# Map the column values to ensure consistent True/False representation\n",
    "df['EQUITY_SWAP_INVOLVED'] = df['EQUITY_SWAP_INVOLVED'].replace({\n",
    "    'false': 'False',\n",
    "    '0': 'False',\n",
    "    '1': 'True',\n",
    "    'true': 'True',\n",
    "    'False': 'False',\n",
    "    'True': 'True'\n",
    "})\n",
    "# Convert the column to boolean type\n",
    "df['EQUITY_SWAP_INVOLVED'] = df['EQUITY_SWAP_INVOLVED'].map({'True': True, 'False': False})\n",
    "# Print unique values to confirm conversion\n",
    "print(df['EQUITY_SWAP_INVOLVED'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for column TRANS_TIMELINESS, E = early, L = late, O = on time\n",
    "print(df['TRANS_TIMELINESS'].unique())\n",
    "df['TRANS_TIMELINESS'] = df['TRANS_TIMELINESS'].replace(np.nan, 'O')\n",
    "print(df['TRANS_TIMELINESS'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove rows where SHRS_OWND_FOLWING_TRANS is nan or TRANS_PRICEPERSHR is nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the column SHRS_OWND_FOLWNG_TRANS and TRANS_PRICEPERSHARE we remove any rows where the value is NaN for either column\n",
    "df = df.dropna(subset=['SHRS_OWND_FOLWNG_TRANS', 'TRANS_PRICEPERSHARE'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print the dataframe summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANS_ACQUIRED_DISP_CD: A = acquired, D = disposed\n",
    "# DIRECT_INDIRECT_OWNERSHIP: D = direct, I = indirect\n",
    "# EQUITY_SWAP_INVOLVED: 0 = false, 1 = true\n",
    "\n",
    "# Print DataFrame information\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b: SUBMISSION.TSV (from different quearters and years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all files and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the files\n",
    "files = [os.path.join('data', 'raw', 'insider_transactions', f'{year}q{quarter}_form345', 'SUBMISSION.tsv')\n",
    "         for year in range(2014, 2018) for quarter in range(1, 5)]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df2 = pd.concat(dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coulmns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep columns: ACCESSION_NUMBER, FILING_DATE, PERIOD_OF_REPORT, ISSUERNAME, ISSUERTRADINGSYMBOL\n",
    "columns_to_keep = ['ACCESSION_NUMBER', 'FILING_DATE', 'PERIOD_OF_REPORT', 'ISSUERNAME', 'ISSUERTRADINGSYMBOL']\n",
    "# Drop columns that are not needed\n",
    "df2.drop(columns=[col for col in df2.columns if col not in columns_to_keep], inplace=True)\n",
    "# Print DataFrame information\n",
    "print(df2.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same company name should have the same trading symbol\n",
    "# if 'ISSUERTRADINGSYMBOL' is nan, we look at its corresponding ISSUERNAME value. \n",
    "# if the corresponding ISSUERNAME is not nan, we can use it to find other rows of the same ISSUERNAME where ISSUERTRADINGSYMBOL is not nan and fill the nan value with the non-nan value.\n",
    "\n",
    "#  if ISSUERNAME is nan, we can't do anything about it. we will just leave it as nan and drop rows where ISSUERTRADINGSYMBOL is nan \n",
    "\n",
    "# Create a mapping of ISSUERNAME to ISSUERTRADINGSYMBOL for non-NaN trading symbols\n",
    "issuer_symbol_map = df2.dropna(subset=['ISSUERTRADINGSYMBOL']).set_index('ISSUERNAME')['ISSUERTRADINGSYMBOL'].to_dict()\n",
    "\n",
    "# Apply the mapping to fill NaN values in ISSUERTRADINGSYMBOL based on ISSUERNAME\n",
    "df2['ISSUERTRADINGSYMBOL'] = df2.apply(\n",
    "    lambda row: issuer_symbol_map.get(row['ISSUERNAME'], row['ISSUERTRADINGSYMBOL']) \n",
    "    if pd.isna(row['ISSUERTRADINGSYMBOL']) and pd.notna(row['ISSUERNAME']) else row['ISSUERTRADINGSYMBOL'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop rows where ISSUERTRADINGSYMBOL is still NaN\n",
    "df2.dropna(subset=['ISSUERTRADINGSYMBOL'], inplace=True)\n",
    "\n",
    "# Print DataFrame info to verify changes\n",
    "# FILING_DATE is when the form was filed to the commission\n",
    "# TRANS_DATE is when the transaction was executed\n",
    "# declaration of intent to trade or smth like that means that PERIOD_OF_REPORT can be before or same data as TRANS_DATE\n",
    "# while filing date is maybe not needed for predictions, the report period can be useful.\n",
    "# we can check if the report period is done before transaction date, indicating clear intent to trade . (maybe we can use delta between the two dates as a feature)\n",
    "\n",
    "print(df2.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c: REPORTING_OWNER.tsv (from different quearters and years)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all files and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the files\n",
    "files = [os.path.join('data', 'raw', 'insider_transactions', f'{year}q{quarter}_form345', 'REPORTINGOWNER.tsv')\n",
    "         for year in range(2014, 2018) for quarter in range(1, 5)]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep='\\t', low_memory=False)\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df3 = pd.concat(dataframes, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep RPTOWNER_RELATIONSHIP and ACCESSION_NUMBER\n",
    "columns_to_keep = ['RPTOWNER_RELATIONSHIP', 'ACCESSION_NUMBER']\n",
    "# Drop columns that are not needed\n",
    "df3.drop(columns=[col for col in df3.columns if col not in columns_to_keep], inplace=True)\n",
    "#drop nan RPTOWNER_RELATIONSHIP\n",
    "df3.dropna(subset=['RPTOWNER_RELATIONSHIP'], inplace=True)\n",
    "# Print DataFrame information\n",
    "print(df3.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d: Joined NONDERIV_TRANS.tsv, SUBMISSION.tsv, REPORTING_OWNER.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4= join df, df2, df3 on ACCESSION_NUMBER\n",
    "df4 = df.merge(df2, on='ACCESSION_NUMBER').merge(df3, on='ACCESSION_NUMBER')\n",
    "# Print DataFrame information\n",
    "print(df4.info())\n",
    "# TRANS_ACQUIRED_DISP_CD: A = acquired, D = disposed\n",
    "# DIRECT_INDIRECT_OWNERSHIP: D = direct, I = indirect\n",
    "# EQUITY_SWAP_INVOLVED: 0 = false, 1 = true\n",
    "# for column TRANS_TIMELINESS, E = early, L = late, O = on time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Stock Prices ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date,Open,High,Low,Close,Volume,OpenInt\n",
    "#  above are the columns in the stock data. We can ignore the OpenInt column as it is not needed.\n",
    "# the folder structure is ../data/raw/stock_data/xxx.us.txt where xxx is the stock symbol.\n",
    "# before the first '.' delimiter, we have the symbol name.\n",
    "# After the second '.' delimiter, we have the country name (us in this case). \n",
    "#  Therefore, the Insider Trading data's ISSUERTRADINGSYMBOL column should match the stock symbol name. The country name here seems irrelevant since in the REPORTINGOWNER.tsv file, we have the country name but that country is like the address of reporting owner (person) and not the stock.\n",
    "#  Therefore, we can ignore the country name in the stock data file name and just match the symbol name.\n",
    "\n",
    "#  let's read all files and extract the symbol name from the file name and store it in a new column called 'SYMBOL'. We will remove OpenInt column as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a list of all the files\n",
    "files = [os.path.join('data', 'raw', 'stock_prices', filename) \n",
    "         for filename in os.listdir(os.path.join('data', 'raw', 'stock_prices')) \n",
    "         if filename.endswith('.txt')]\n",
    "# Read all the files and store them in a list\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    #  extract symbol name from file name which is the string before the first '.' delimiter in the file name\n",
    "    symbol = os.path.basename(file).split('.')[0]\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            temp = pd.read_csv(file, sep=',', low_memory=False)\n",
    "            # Add a new column 'SYMBOL' with the symbol name\n",
    "            temp['SYMBOL'] = symbol\n",
    "            # remove OpenInt column\n",
    "            temp.drop(columns=['OpenInt'], inplace=True)\n",
    "            # filter dates to be from 2014 till 2017 (inclusive and all months)\n",
    "            temp = temp[temp['Date'].str.startswith('2014') | temp['Date'].str.startswith('2015') | temp['Date'].str.startswith('2016') | temp['Date'].str.startswith('2017')]\n",
    "            dataframes.append(temp)\n",
    "        except Exception as e:\n",
    "            print(f'Error reading {file}: {e}')# as an example: Error reading ..\\data\\raw\\stock_prices\\accp.us.txt: No columns to parse from file (empty data file)\n",
    "\n",
    "    else:\n",
    "        print(f'File {file} does not exist')\n",
    "# Concatenate all DataFrames into one DataFrame\n",
    "df5 = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Merging Insider Trading and Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the stocks prices dataset has  Symbol and Date columns (Date and SYMBOL)\n",
    "#  the insider trading data has the transaction date and the stock symbol name (TRANS_DATE and ISSUERTRADINGSYMBOL)TRANS_DATE HAS THE FORM 13-MAR-2014\n",
    "\n",
    "# therefore, we can join the insider trading data with the stock prices data on the stock symbol name and the transaction date. dATE HAS THE FORM 2014-01-23    \n",
    "# df4 is the insider trading data and df5 is the stock prices data\n",
    "\n",
    "# Ensure both dataframes have symbol columns in the same case (e.g., uppercase)\n",
    "df4['ISSUERTRADINGSYMBOL'] = df4['ISSUERTRADINGSYMBOL'].str.upper()\n",
    "df5['SYMBOL'] = df5['SYMBOL'].str.upper()\n",
    "\n",
    "# Convert TRANS_DATE to the same format as Date in df5\n",
    "df4['TRANS_DATE'] = pd.to_datetime(df4['TRANS_DATE'], format='%d-%b-%Y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge the insider trading data with the stock prices data on the stock symbol name and the transaction date\n",
    "merged_df = pd.merge(df4, df5, left_on=['ISSUERTRADINGSYMBOL', 'TRANS_DATE'], right_on=['SYMBOL', 'Date'], how='inner')\n",
    "\n",
    "# Print the merged DataFrame information\n",
    "print(merged_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Saving Interim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 1,322,820 million rows for all the insider trading data files.(7,877 unique symbols)\n",
    "# We have 5,442,556 rows for the stocks price data files. (7,163 unique symbols)\n",
    "# merging both based on the stock symbol name and the transaction date, we have 978,647 rows. (4,450 unique symbols)\n",
    "# from 2014 to 2017, we have 1,043 working business days.\n",
    "\n",
    "# naturally, the insider trading data is less than the stock prices data as not all companies have insider trading data.\n",
    "# the merged data could be useful for predicting stock prices based on insider trading data.(direct daily relationship between insider trading data and stock prices)\n",
    "# but there will be many more data points in the stock prices that have no corresponding insider trading data. (indirect relationship between insider trading data and stock prices).\n",
    "# in our plot, we can first plot all stocks prices and then color-code the points that have insider trading data vs those that don't have insider trading data.\n",
    "\n",
    "\n",
    "#  for now, let's save the df4,  to the folder path ../data/interim/insider_transactions\n",
    "#  let's save the df5 to the folder path ../data/interim/stock_prices\n",
    "# let's save the merged_df to the folder path ../data/interim/merged_insider_transactions_stock_prices\n",
    "# we save using paths and os packages that work on all operating systems.\n",
    "#  if the folders do not exist, we create them.\n",
    "\n",
    "# Define the folder paths\n",
    "insider_transactions_path = os.path.join('data', 'interim', 'insider_transactions')\n",
    "stock_prices_path = os.path.join('data', 'interim', 'stock_prices')\n",
    "merged_path = os.path.join('data', 'interim', 'merged_insider_transactions_stock_prices')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(insider_transactions_path, exist_ok=True)\n",
    "os.makedirs(stock_prices_path, exist_ok=True)\n",
    "os.makedirs(merged_path, exist_ok=True)\n",
    "\n",
    "# Save the DataFrames to the respective paths\n",
    "df4.to_csv(os.path.join(insider_transactions_path, 'interim_insider_transactions.csv'), index=False)\n",
    "df5.to_csv(os.path.join(stock_prices_path, 'interim_stock_prices.csv'), index=False)\n",
    "merged_df.to_csv(os.path.join(merged_path, 'interim_merged_insider_transactions_stock_prices.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
